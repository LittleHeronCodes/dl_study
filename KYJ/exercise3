## toy code3: https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%206%20-%20Lesson%202%20-%20Notebook.ipynb
## exercise3: https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Exercises/Exercise%203%20-%20Convolutions/Exercise%203%20-%20Question.ipynb#scrollTo=sfQRyaJWAIdg

import tensorflow as tf

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('acc')>0.998):
      print("\nReached 99.8% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train=x_train.reshape(60000, 28, 28, 1)
x_train=x_train / 255.0
x_test = x_test.reshape(10000, 28, 28, 1)
x_test=x_test/255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)), ## 26,26,64
  ## changable (generate 32 filters, 3 by 3, and their act. is relu, which means the nega. values will be thrown (a)way)
  ## because of 3,3 filters, 28,28 -> 26x26 pixels can be conversed.
  tf.keras.layers.MaxPooling2D(2, 2), ## 13,13,64 (26/2=13)
  ## changable (for every four pixels, the biggest one will survive)
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)), ## 11,11,64
  tf.keras.layers.MaxPooling2D(2, 2), ## 5,5,64 (11/2=5)
  tf.keras.layers.Flatten(), ## 5x5x64 = 1,600
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
## model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])
test_loss, test_acc = model.evaluate(x_test, y_test)
